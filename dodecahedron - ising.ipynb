{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh, qr, null_space\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from scipy.sparse import kron, identity, csr_matrix, csc_matrix, lil_matrix, dok_matrix, issparse, coo_matrix\n",
    "from scipy.sparse.linalg import eigsh, eigs\n",
    "from scipy.optimize import curve_fit\n",
    "from qutip import Qobj, ptrace, entropy_vn, qeye, tensor\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import sympy as sp\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauli_x():\n",
    "    \"\"\"Pauli X matrix.\"\"\"\n",
    "    return np.array([[0, 1], [1, 0]])\n",
    "\n",
    "def pauli_z():\n",
    "    \"\"\"Pauli Z matrix.\"\"\"\n",
    "    return np.array([[1, 0], [0, -1]])\n",
    "\n",
    "def dodecahedral_bonds(): #20 vertices\n",
    "    \"\"\"\n",
    "    Defines the connectivity of a true 20-vertex dodecahedral molecular structure.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple (i, j) represents a bond between spin i and spin j.\n",
    "    \"\"\"\n",
    "    bonds = [\n",
    "    (0, 13), (0, 14), (0, 15),\n",
    "    (1, 4), (1, 5), (1, 12),\n",
    "    (2, 6), (2, 13), (2, 18),\n",
    "    (3, 7), (3, 14), (3, 19),\n",
    "    (4, 10), (4, 18),\n",
    "    (5, 11), (5, 19),\n",
    "    (6, 10), (6, 15),\n",
    "    (7, 11), (7, 15),\n",
    "    (8, 9), (8, 13), (8, 16),\n",
    "    (9, 14), (9, 17),\n",
    "    (10, 11),\n",
    "    (12, 16), (12, 17),\n",
    "    (16, 18),\n",
    "    (17, 19)\n",
    "]\n",
    "\n",
    "    return bonds\n",
    "\n",
    "\n",
    "def transverse_field_ising_dodecahedral(N, J, h):\n",
    "    \"\"\"\n",
    "    Constructs the Hamiltonian for the transverse field Ising model on a dodecahedral molecular structure.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of spins (should match the dodecahedral molecule, typically N=12).\n",
    "        J (float): Interaction strength.\n",
    "        h (float): Transverse field strength.\n",
    "    \n",
    "    Returns:\n",
    "        H (scipy.sparse.csr_matrix): The Hamiltonian matrix in sparse format.\n",
    "    \"\"\"\n",
    "    if N != 20:\n",
    "        raise ValueError(\"Dodecahedral molecules typically have N = 20 sites.\")\n",
    "\n",
    "    # Sparse identity matrix\n",
    "    I = identity(2, format=\"csr\")\n",
    "    \n",
    "    # Pauli matrices as sparse matrices\n",
    "    X = csr_matrix(pauli_x())\n",
    "    Z = csr_matrix(pauli_z())\n",
    "    \n",
    "    # Initialize the Hamiltonian\n",
    "    H = csr_matrix((2**N, 2**N), dtype=np.float64)\n",
    "\n",
    "    # Get dodecahedral bonds\n",
    "    bonds = dodecahedral_bonds()\n",
    "\n",
    "    # Interaction term: J * sigma_i^x * sigma_j^x for dodecahedral connectivity\n",
    "    for i, j in bonds:\n",
    "        term = 1\n",
    "        for k in range(N):\n",
    "            if k == i or k == j:\n",
    "                term = kron(term, X, format=\"csr\")\n",
    "            else:\n",
    "                term = kron(term, I, format=\"csr\")\n",
    "        H += J * term\n",
    "    \n",
    "    # Transverse field term: -h * sigma_i^z\n",
    "    for i in range(N):\n",
    "        term = 1\n",
    "        for j in range(N):\n",
    "            if j == i:\n",
    "                term = kron(term, Z, format=\"csr\")\n",
    "            else:\n",
    "                term = kron(term, I, format=\"csr\")\n",
    "        H += -h * term\n",
    "    \n",
    "    return H\n",
    "\n",
    "def ising_dodecahedron(N, J):\n",
    "    \"\"\"\n",
    "    Constructs the Hamiltonian for the transverse field Ising model on a dodecahedral molecular structure without transverse field.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of spins (should match the dodecahedral molecule, typically N=20).\n",
    "        J (float): Interaction strength.\n",
    "    \n",
    "    Returns:\n",
    "        H (scipy.sparse.csr_matrix): The Hamiltonian matrix in sparse format.\n",
    "    \"\"\"\n",
    "    if N != 20:\n",
    "        raise ValueError(\"Dodecahedral molecules typically have N = 20 sites.\")\n",
    "\n",
    "    # Sparse identity matrix\n",
    "    I = identity(2, format=\"csr\")\n",
    "    \n",
    "    # Pauli matrices as sparse matrices\n",
    "    X = csr_matrix(pauli_x())\n",
    "    \n",
    "    # Initialize the Hamiltonian\n",
    "    H = csr_matrix((2**N, 2**N), dtype=np.float64)\n",
    "\n",
    "    # Get Dodecahedron bonds\n",
    "    bonds = dodecahedral_bonds()\n",
    "\n",
    "    # Interaction term: J * sigma_i^x * sigma_j^x for Dodecahedron connectivity\n",
    "    for i, j in bonds:\n",
    "        term = 1\n",
    "        for k in range(N):\n",
    "            if k == i or k == j:\n",
    "                term = kron(term, X, format=\"csr\")\n",
    "            else:\n",
    "                term = kron(term, I, format=\"csr\")\n",
    "        H += J * term\n",
    "    \n",
    "    return H\n",
    "\n",
    "def transverse_field_dodecahedral(N, h):\n",
    "    \"\"\"\n",
    "    Constructs the Hamiltonian for the transverse field Ising model on a dodecahedral molecular structure.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of spins (should match the icosahedral molecule, typically N=20).\n",
    "        J (float): Interaction strength.\n",
    "        h (float): Transverse field strength.\n",
    "    \n",
    "    Returns:\n",
    "        H (scipy.sparse.csr_matrix): The Hamiltonian matrix in sparse format.\n",
    "    \"\"\"\n",
    "    if N != 20:\n",
    "        raise ValueError(\"Dodecahedral molecules typically have N = 20 sites.\")\n",
    "\n",
    "    # Sparse identity matrix\n",
    "    I = identity(2, format=\"csr\")\n",
    "    \n",
    "    # Pauli matrices as sparse matrices\n",
    "    Z = csr_matrix(pauli_z())\n",
    "    \n",
    "    # Initialize the Hamiltonian\n",
    "    H = csr_matrix((2**N, 2**N), dtype=np.float64)\n",
    "\n",
    "    # Get dodecahedral bonds\n",
    "    bonds = dodecahedral_bonds()\n",
    "\n",
    "    # Transverse field term: -h * sigma_i^z\n",
    "    for i in range(N):\n",
    "        term = 1\n",
    "        for j in range(N):\n",
    "            if j == i:\n",
    "                term = kron(term, Z, format=\"csr\")\n",
    "            else:\n",
    "                term = kron(term, I, format=\"csr\")\n",
    "        H += -h * term\n",
    "    \n",
    "    return H\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "'''\n",
    "def partial_trace_qubit(rho, keep, dims):\n",
    "    \"\"\"Compute the partial trace of a density matrix of qubits.\"\"\"\n",
    "    keep_dims = np.prod([dims[i] for i in keep])\n",
    "    trace_dims = np.prod([dims[i] for i in range(len(dims)) if i not in keep])\n",
    "    rho = rho.reshape([keep_dims, trace_dims, keep_dims, trace_dims])\n",
    "    return np.trace(rho, axis1=1, axis2=3).reshape([keep_dims, keep_dims])\n",
    "\n",
    "def partial_trace_qubit_torch(rho, keep, dims):\n",
    "    \"\"\"Compute the partial trace of a density matrix of qubits using PyTorch.\"\"\"\n",
    "    keep_dims = torch.prod(torch.tensor([dims[i] for i in keep]))\n",
    "    trace_dims = torch.prod(torch.tensor([dims[i] for i in range(len(dims)) if i not in keep]))\n",
    "    rho = rho.view(keep_dims, trace_dims, keep_dims, trace_dims)\n",
    "    # Compute the partial trace\n",
    "    traced_rho = torch.zeros((keep_dims, keep_dims), dtype=rho.dtype)\n",
    "    for i in range(trace_dims):\n",
    "        traced_rho += rho[:, i, :, i]\n",
    "    #return traced_rho.view(keep_dims, keep_dims)\n",
    "    return traced_rho'''\n",
    "\n",
    "def isket_numpy(arr):\n",
    "    \"\"\"\n",
    "    Check if a NumPy array is a ket (column vector).\n",
    "\n",
    "    Parameters:\n",
    "    - arr: np.ndarray, the array to check.\n",
    "\n",
    "    Returns:\n",
    "    - bool, True if the array is a ket, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        raise ValueError(\"Input must be a NumPy array\")\n",
    "\n",
    "    shape = arr.shape\n",
    "\n",
    "    if len(shape) == 2 and shape[1] == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ptrace_numpy(Q, sel, dims): # numpy function adapted from ptrace of qutip\n",
    "    \"\"\"\n",
    "    Compute the partial trace of a density matrix of qubits using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: numpy object, the quantum object (density matrix or state vector).\n",
    "    - sel: list of int, indices of the subsystems to keep.\n",
    "    - dims: list of int, dimensions of the subsystems.\n",
    "\n",
    "    Returns:\n",
    "    - numpy object, the reduced density matrix after tracing out the specified subsystems.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the subsystems\n",
    "    rd = np.asarray(dims[0], dtype=np.int32).ravel()\n",
    "    nd = len(rd)\n",
    "    \n",
    "    # Ensure sel is a sorted array of indices\n",
    "    if isinstance(sel, int):\n",
    "        sel = np.array([sel])\n",
    "    else:\n",
    "        sel = np.asarray(sel)\n",
    "    sel = list(np.sort(sel))\n",
    "    \n",
    "    # Dimensions of the subsystems to keep\n",
    "    dkeep = (rd[sel]).tolist()\n",
    "    \n",
    "    # Indices of the subsystems to trace out\n",
    "    qtrace = list(set(np.arange(nd)) - set(sel))\n",
    "    \n",
    "    # Dimensions of the subsystems to trace out\n",
    "    dtrace = (rd[qtrace]).tolist()\n",
    "    \n",
    "    # Reshape the density matrix or state vector\n",
    "    rd = list(rd)\n",
    "    if isket_numpy(Q):\n",
    "        # Reshape and transpose for state vector\n",
    "        vmat = (Q\n",
    "                .reshape(rd)\n",
    "                .transpose(sel + qtrace)\n",
    "                .reshape([np.prod(dkeep), np.prod(dtrace)]))\n",
    "        # Compute the reduced density matrix\n",
    "        rhomat = vmat.dot(vmat.conj().T)\n",
    "    else:\n",
    "        # Reshape and transpose for density matrix\n",
    "        rhomat = np.trace(Q\n",
    "                          .reshape(rd + rd)\n",
    "                          .transpose(qtrace + [nd + q for q in qtrace] +\n",
    "                                     sel + [nd + q for q in sel])\n",
    "                          .reshape([np.prod(dtrace),\n",
    "                                    np.prod(dtrace),\n",
    "                                    np.prod(dkeep),\n",
    "                                    np.prod(dkeep)]))\n",
    "    return rhomat\n",
    "\n",
    "\n",
    "def ptrace_sparse(psi_sparse, keep, dims):\n",
    "    \"\"\"\n",
    "    Compute the partial trace over arbitrary subsystems using sparse matrix operations.\n",
    "\n",
    "    Args:\n",
    "        psi_sparse (scipy.sparse matrix): Full density matrix of shape (D, D), where D = product(dims)\n",
    "        keep (list of int): Subsystems to keep (indices, 0-indexed)\n",
    "        dims (list of int): List of subsystem dimensions, e.g., [2]*n for n qubits\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Reduced density matrix over kept subsystems\n",
    "    \"\"\"\n",
    "    if not issparse(psi_sparse):\n",
    "        raise ValueError(\"psi_sparse must be a scipy.sparse matrix\")\n",
    "    n = len(dims)\n",
    "    D = np.prod(dims)\n",
    "    if psi_sparse.shape != (D, D):\n",
    "        raise ValueError(\"Density matrix shape does not match dims\")\n",
    "    trace = [i for i in range(n) if i not in keep]\n",
    "    d_keep = np.prod([dims[i] for i in keep])\n",
    "    # Prepare output\n",
    "    data = []\n",
    "    row_idx = []\n",
    "    col_idx = []\n",
    "\n",
    "    # Precompute bit masks\n",
    "    def idx_to_bits(idx):\n",
    "        return np.array(list(np.binary_repr(idx, width=n))).astype(int)\n",
    "    \n",
    "\n",
    "    psi_sparse = psi_sparse.tocoo()\n",
    "    for i, j, val in zip(psi_sparse.row, psi_sparse.col, psi_sparse.data):\n",
    "        bi = idx_to_bits(i)\n",
    "        bj = idx_to_bits(j)\n",
    "\n",
    "\n",
    "        # Only sum terms where traced-out subsystems agree\n",
    "        if np.all(bi[trace] == bj[trace]):\n",
    "            # Extract kept bits and convert to reduced indices\n",
    "            #print('condition met for i, j:', i, j)\n",
    "            i_red_bits = bi[keep]\n",
    "            j_red_bits = bj[keep]\n",
    "            i_red = int(\"\".join(i_red_bits.astype(str)), 2)\n",
    "            j_red = int(\"\".join(j_red_bits.astype(str)), 2)\n",
    "\n",
    "\n",
    "            data.append(val)\n",
    "            row_idx.append(i_red)\n",
    "            col_idx.append(j_red)\n",
    "    \n",
    "    return coo_matrix((data, (row_idx, col_idx)), shape=(d_keep, d_keep)).tocsr()\n",
    "\n",
    "\n",
    "def isket_torch(arr):\n",
    "    \"\"\"\n",
    "    Check if a PyTorch tensor is a ket (column vector).\n",
    "\n",
    "    Parameters:\n",
    "    - arr: torch.Tensor, the array to check.\n",
    "\n",
    "    Returns:\n",
    "    - bool, True if the array is a ket, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(arr, torch.Tensor):\n",
    "        raise ValueError(\"Input must be a PyTorch tensor\")\n",
    "\n",
    "    shape = arr.shape\n",
    "\n",
    "    if len(shape) == 2 and shape[1] == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ptrace_torch(Q, sel, dims): # torch function adapted from ptrace of qutip\n",
    "    \"\"\"\n",
    "    Compute the partial trace of a density matrix of qubits using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: torch.Tensor, the quantum object (density matrix or state vector).\n",
    "    - sel: list of int, indices of the subsystems to keep.\n",
    "    - dims: list of int, dimensions of the subsystems.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor, the reduced density matrix after tracing out the specified subsystems.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the subsystems\n",
    "    rd = torch.tensor(dims[0], dtype=torch.int32).flatten()\n",
    "    nd = len(rd)\n",
    "    #print(\"rd\", rd)\n",
    "    #print(\"nd\", nd)\n",
    "    \n",
    "    # Ensure sel is a sorted array of indices\n",
    "    if isinstance(sel, int):\n",
    "        sel = torch.tensor([sel])\n",
    "    else:\n",
    "        sel = torch.tensor(sel)\n",
    "    sel = torch.sort(sel).values.tolist()\n",
    "    \n",
    "    # Dimensions of the subsystems to keep\n",
    "    dkeep = rd[sel].tolist()\n",
    "    \n",
    "    # Indices of the subsystems to trace out\n",
    "    qtrace = list(set(range(nd)) - set(sel))\n",
    "    \n",
    "    # Dimensions of the subsystems to trace out\n",
    "    dtrace = rd[qtrace].tolist()\n",
    "    \n",
    "    # Reshape the density matrix or state vector\n",
    "    rd = rd.tolist()\n",
    "    if isket_torch(Q):\n",
    "        # Reshape and transpose for state vector\n",
    "        reshaped_Q = Q.reshape(rd)\n",
    "        #print(reshaped_Q.shape)\n",
    "        transposed_Q = reshaped_Q.permute(sel + qtrace)\n",
    "        #print(transposed_Q.shape)\n",
    "        vmat = transposed_Q.reshape([torch.prod(torch.tensor(dkeep)), torch.prod(torch.tensor(dtrace))])\n",
    "        #print(vmat.shape)\n",
    "        # Compute the reduced density matrix\n",
    "        rhomat = vmat @ vmat.conj().T\n",
    "        #print(rhomat.shape)\n",
    "    else:\n",
    "        # Reshape and transpose for density matrix\n",
    "        reshaped_Q = Q.reshape(rd + rd)\n",
    "        #print(\"reshaped_Q\", reshaped_Q.shape)\n",
    "        transposed_Q = reshaped_Q.permute(qtrace + [nd + q for q in qtrace] + sel + [nd + q for q in sel])\n",
    "        #print(\"transposed_Q\", transposed_Q.shape)\n",
    "        reshaped_transposed_Q = transposed_Q.reshape([torch.prod(torch.tensor(dtrace)), torch.prod(torch.tensor(dtrace)), torch.prod(torch.tensor(dkeep)), torch.prod(torch.tensor(dkeep))])\n",
    "        #print(\"reshaped_transposed_Q\", reshaped_transposed_Q.shape)\n",
    "        #rhomat = torch.trace(reshaped_transposed_Q)\n",
    "        rhomat = torch.einsum('iikl->kl', reshaped_transposed_Q)\n",
    "        # Trace out the first two dimensions\n",
    "        #rhomat = torch.zeros((torch.prod(torch.tensor(dkeep)), torch.prod(torch.tensor(dkeep))), dtype=Q.dtype)\n",
    "        #for i in range(reshaped_transposed_Q.shape[0]):\n",
    "        #    for j in range(reshaped_transposed_Q.shape[1]):\n",
    "        #        rhomat += reshaped_transposed_Q[i, j, :, :]\n",
    "        #print(\"rhomat\", rhomat.shape)\n",
    "    return rhomat\n",
    "\n",
    "def entanglement_entropy(psi, subsystem, total_size):\n",
    "\n",
    "    '''Computes the bipartite entanglement entropy of a pure state.\n",
    "    \n",
    "    Parameters:\n",
    "    psi : np.array\n",
    "        The wavefunction (state vector) of the full system.\n",
    "    subsystem_size : int\n",
    "        The number of qubits in subsystem A.\n",
    "    total_size : int\n",
    "        The total number of qubits in the system.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The von Neumann entanglement entropy S_A.'''\n",
    "    \n",
    "    psi_matrix =  np.outer(psi, psi.conj())\n",
    "\n",
    "    # Compute the reduced density matrix rho_A = Tr_B(|psi><psi|)\n",
    "    rho_A = ptrace_numpy(psi_matrix, subsystem, [[2]*total_size, [2]*total_size])  # Partial trace over B\n",
    "    \n",
    "    # Compute eigenvalues of rho_A\n",
    "    eigenvalues = np.linalg.eigvalsh(rho_A)\n",
    "    \n",
    "    # Filter out zero eigenvalues to avoid numerical issues in log calculation\n",
    "    eigenvalues = eigenvalues[eigenvalues > 0]\n",
    "    \n",
    "    # Compute von Neumann entropy S_A = -Tr(rho_A log rho_A)\n",
    "    entropy = -np.sum(eigenvalues * np.log2(eigenvalues))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def entanglement_entropy_torch(psi, subsystem, total_size):\n",
    "    \"\"\"\n",
    "    Computes the bipartite entanglement entropy of a pure state using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - psi: torch.Tensor (complex), the wavefunction (state vector) of the full system.\n",
    "    - subsystem_size: int, the number of qubits in subsystem A.\n",
    "    - total_size: int, the total number of qubits in the system.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor (scalar), the von Neumann entanglement entropy S_A.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(psi, torch.Tensor):\n",
    "        psi = torch.tensor(psi, dtype=torch.complex64)\n",
    "    \n",
    "    # Ensure psi is normalized\n",
    "    psi = psi / torch.norm(psi)\n",
    "\n",
    "    # Compute the density matrix |psi><psi|\n",
    "    psi_matrix = torch.outer(psi, psi.conj())\n",
    "\n",
    "    # Compute the reduced density matrix rho_A = Tr_B(|psi><psi|)\n",
    "    rho_A = ptrace_torch(psi_matrix, subsystem, [[2] * total_size, [2] * total_size])  # Partial trace over B\n",
    "\n",
    "    #rho_A = rho_A.to(dtype=torch.float64)\n",
    "    \n",
    "    # Compute eigenvalues of rho_A\n",
    "    eigvals = torch.linalg.eigvalsh(rho_A)\n",
    "\n",
    "    # Filter out zero eigenvalues to avoid numerical issues in log calculation\n",
    "    eigvals = eigvals[eigvals > 0]\n",
    "\n",
    "    # Compute von Neumann entropy S_A = -Tr(rho_A log rho_A)\n",
    "    entropy = -torch.sum(eigvals * torch.log2(eigvals))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def entanglement_entropy_qutip(psi, subsystem, total_size):\n",
    "    \n",
    "    # Convert the wavefunction to a QuTiP Qobj\n",
    "    density_matrix = np.outer(psi, psi.conj())\n",
    "    density_matrix_qobj = Qobj(density_matrix, dims=[[2]*total_size, [2]*total_size])\n",
    "\n",
    "    rho_A = ptrace(density_matrix_qobj, subsystem)\n",
    "    # Compute the von Neumann entropy S_A\n",
    "    entropy = entropy_vn(rho_A, base=2)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def entanglement_entropy_np_ptrace(rdm):\n",
    "    # rdm already computed and converted to numpy\n",
    "    # Compute eigenvalues of rho_A\n",
    "    eigenvalues = np.linalg.eigvalsh(rdm)\n",
    "    \n",
    "    # Filter out zero eigenvalues to avoid numerical issues in log calculation\n",
    "    eigenvalues = eigenvalues[eigenvalues > 0]\n",
    "    \n",
    "    # Compute von Neumann entropy S_A = -Tr(rho_A log rho_A)\n",
    "    entropy = -np.sum(eigenvalues * np.log2(eigenvalues))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def entanglement_entropy_torch_ptrace(rdm):\n",
    "\n",
    "    eigvals = torch.linalg.eigvalsh(rdm)\n",
    "    eigvals = eigvals[eigvals > 0]\n",
    "    entropy = -torch.sum(eigvals * torch.log2(eigvals))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def entanglement_entropy_qutip_torch(psi, N):\n",
    "    \"\"\"\n",
    "    Compute the von Neumann entanglement entropy using qutip.\n",
    "\n",
    "    Parameters:\n",
    "    - psi: torch.Tensor (complex), state vector of a quantum system.\n",
    "    - N: int, total number of qubits.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor (scalar), von Neumann entropy.\n",
    "    \"\"\"\n",
    "    # Ensure psi is normalized\n",
    "    psi = psi / torch.norm(psi)\n",
    "\n",
    "    # Convert PyTorch tensor to NumPy for QuTiP\n",
    "    psi_np = psi.detach().numpy()\n",
    "\n",
    "    rho_np = np.outer(psi_np, psi_np.conj())\n",
    "    rho_qobj = Qobj(rho_np, dims=[[2] * N, [2] * N])\n",
    "\n",
    "    rho_A = ptrace(rho_qobj, list(range(N // 2)))\n",
    "\n",
    "    # Compute von Neumann entropy\n",
    "    entropy = entropy_vn(rho_A, base=2)  # Compute in log base 2\n",
    "\n",
    "    # Convert back to PyTorch tensor to allow gradient flow\n",
    "    return torch.tensor(entropy, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Define the linear combination function - numpy\n",
    "def linear_combination_np(coeffs, psis):\n",
    "    # Ensure psis are numpy tensors\n",
    "    psi_np = [np.array(psi) for psi in psis]\n",
    "    # Compute the linear combination in PyTorch\n",
    "    psi = sum(c * psi for c, psi in zip(coeffs, psis))\n",
    "    \n",
    "    return psi\n",
    "\n",
    "# Define the linear combination function - torch\n",
    "def linear_combination(coeffs, psis):\n",
    "    # Ensure psis are PyTorch tensors\n",
    "    psis_torch = [torch.tensor(psi, dtype=torch.complex64) if not isinstance(psi, torch.Tensor) else psi for psi in psis]\n",
    "    \n",
    "    # Compute the linear combination in PyTorch\n",
    "    psi_torch = sum(c * psi for c, psi in zip(coeffs, psis_torch))\n",
    "    \n",
    "    return psi_torch\n",
    "\n",
    "# Define the linear combination function - torch but after computing the ptrace of outer products of scars\n",
    "def linear_combination_outer(coeffs, outs):\n",
    "    # Ensure outs are PyTorch tensors\n",
    "    outs_torch = [torch.tensor(out, dtype=torch.complex64) if not isinstance(out, torch.Tensor) else out for out in outs]\n",
    "    torch_coeffs = torch.tensor(coeffs, dtype=torch.complex64)\n",
    "\n",
    "    # Compute the PyTorch tensor of out_coeffs which is the product of all possible combinations of c_i^* times c_j\n",
    "    out_coeffs = torch.zeros((len(torch_coeffs), len(torch_coeffs)), dtype=torch.complex64)\n",
    "    for i in range(len(torch_coeffs)):\n",
    "        for j in range(len(torch_coeffs)):\n",
    "            out_coeffs[i, j] = torch.conj(torch_coeffs[i]) * torch_coeffs[j]\n",
    "    \n",
    "    # Compute the linear combination in PyTorch\n",
    "    lin_torch = sum(out_coeffs[i, j] * outs_torch[i] for i in range(len(coeffs)) for j in range(len(coeffs)))\n",
    "    \n",
    "    return lin_torch\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Function to apply permutation to a given spin configuration\n",
    "def apply_permutation(state_bits, N, perm):\n",
    "    new_bits = [0] * N\n",
    "    for i in range(N):\n",
    "        new_bits[perm[i]] = state_bits[i]  # Map value at index i to perm[i]\n",
    "    return new_bits\n",
    "\n",
    "# Function to convert an index to its binary representation\n",
    "def index_to_binary(index, num_qubits):\n",
    "    return format(index, f'0{num_qubits}b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20  # Number of spins\n",
    "J = 1.0  # Interaction strength\n",
    "h = 3.0  # Transverse field strength # this is the value in the paper. maybe try other values too, including the critical value one (h=J=1)\n",
    "\n",
    "keep_qubits = [0, 3, 5, 7, 11, 14, 15, 19]\n",
    "\n",
    "trace_qubits = [i for i in range(N) if i not in keep_qubits]\n",
    "\n",
    "\n",
    "# Assuming transverse_field_ising is defined and returns a sparse Hermitian matrix\n",
    "H = transverse_field_ising_dodecahedral(N, J, h)\n",
    "\n",
    "#print(f\"Hamiltonian shape: {H.shape}\")\n",
    "#print(f\"Non-zero elements in H: {H.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find eigenvalues around -18 (based on your red_indices being around -18)\n",
    "target_energy = 0.0\n",
    "k = 1  # Number of eigenvalues to find\n",
    "\n",
    "eigenvalues_subset, eigenvectors_subset = eigsh(H, k=k, sigma=target_energy, which='LM')\n",
    "print(f\"Found {len(eigenvalues_subset)} eigenvalues near {target_energy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for integer eigenvalues \n",
    "\n",
    "# H is a sparse Hermitian matrix (CSR format)\n",
    "# k=1 for ground state, which is the lowest eigenvalue\n",
    "# 'SA' means smallest algebraic (lowest) eigenvalue\n",
    "gs_eigenvalue, gs_eigenvector = eigsh(H, k=1, which='SA')\n",
    "\n",
    "print(\"Ground state eigenvalue:\", gs_eigenvalue[0])\n",
    "print(\"Ground state eigenvector shape:\", gs_eigenvector[:, 0].shape)\n",
    "\n",
    "# Now shift H by lambda*I and compute lowest eigenvalue\n",
    "lambdas = [18]\n",
    "for lam in lambdas:\n",
    "    H0 = H - lam * identity(H.shape[0], format='csr')\n",
    "    H_shifted = (H0 @ H0).tocsr()\n",
    "    eigvals_squared, eigvecs = eigsh(H_shifted, k=1, which='SA')\n",
    "    \n",
    "    # Convert back to original eigenvalue\n",
    "    # The eigenvalue of (H - lam*I)^2 is (original_eigenvalue - lam)^2\n",
    "    # So original_eigenvalue = lam Â± sqrt(eigenvalue_of_squared_matrix)\n",
    "    sqrt_eigval = np.sqrt(eigvals_squared[0])\n",
    "    candidate1 = lam + sqrt_eigval\n",
    "    candidate2 = lam - sqrt_eigval\n",
    "    \n",
    "    print(f\"Eigenvalue of (H - {lam}*I)^2: {eigvals_squared[0]}\")\n",
    "    print(f\"Possible original eigenvalues: {candidate1:.6f} or {candidate2:.6f}\")\n",
    "    \n",
    "    # To determine which is correct, you could check which is closer to known eigenvalues\n",
    "    # or use the eigenvector to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All eigenvalues of H\n",
    "# Compute all eigenvalues (and corresponding eigenvectors)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(H.toarray())\n",
    "\n",
    "print(f\"Eigenvalues of H: {np.sort(eigenvalues)}\")\n",
    "\n",
    "# Count the number of (near-)zero components in each eigenvector\n",
    "zero_threshold = 1e-6\n",
    "zero_counts = []\n",
    "for i in range(eigenvectors.shape[1]):\n",
    "    num_zeros = np.sum(np.abs(eigenvectors[:, i]) < zero_threshold)\n",
    "    zero_counts.append(num_zeros)\n",
    "    print(f\"Eigenvector {i}: {num_zeros} components < {zero_threshold}\")\n",
    "\n",
    "# Optionally, print a summary\n",
    "print(\"Zero count distribution:\", Counter(zero_counts))\n",
    "\n",
    "# Check if each eigenvector is complex or real\n",
    "for i in range(eigenvectors.shape[1]):\n",
    "    vec = eigenvectors[:, i]\n",
    "    if np.any(np.abs(vec.imag) > 1e-14):\n",
    "        print(f\"Eigenvector {i} is complex.\")\n",
    "    #else:\n",
    "    #    print(f\"Eigenvector {i} is real.\")\n",
    "\n",
    "print(f\"Eigenvalues of H: {np.sort(eigenvalues)}\")\n",
    "\n",
    "# Count the number of (near-)zero components in each eigenvector\n",
    "zero_threshold = 1e-6\n",
    "zero_counts = []\n",
    "for i in range(eigenvectors.shape[1]):\n",
    "    num_zeros = np.sum(np.abs(eigenvectors[:, i]) < zero_threshold)\n",
    "    zero_counts.append(num_zeros)\n",
    "    print(f\"Eigenvector {i}: {num_zeros} components < {zero_threshold}\")\n",
    "\n",
    "# Optionally, print a summary\n",
    "print(\"Zero count distribution:\", Counter(zero_counts))\n",
    "\n",
    "# Check if each eigenvector is complex or real\n",
    "for i in range(eigenvectors.shape[1]):\n",
    "    vec = eigenvectors[:, i]\n",
    "    if np.any(np.abs(vec.imag) > 1e-14):\n",
    "        print(f\"Eigenvector {i} is complex.\")\n",
    "    #else:\n",
    "    #    print(f\"Eigenvector {i} is real.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### RDMS + EE #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scars + tf field - sparse\n",
    "\n",
    "\n",
    "for i in tqdm(red_indices):\n",
    "    # Construct the density matrix for each eigenvector (as sparse)\n",
    "    min_eigenvector = eigenvectors[:, i]\n",
    "    print(\"Nonzero elements in state vector (tol=1e-12):\", np.sum(np.abs(min_eigenvector) > 1e-6))\n",
    "    min_eigenvector[np.abs(min_eigenvector) < 1e-7] = 0\n",
    "    min_eig_sparse = csr_matrix(min_eigenvector.reshape(-1, 1))  # Convert to sparse column vector\n",
    "    \n",
    "    # Apply the transverse field operator to the sparse vector\n",
    "    vecs = transverse_field_dodecahedral(N, h) @ min_eig_sparse\n",
    "    vecs = vecs.toarray()  # Convert to dense array for further processing\n",
    "    \n",
    "    # Print information about the result\n",
    "    print(f\"Shape of result: {vecs.shape}\")\n",
    "    print(f\"for {i}-th eigenvector: Number of nonzero elements in result: {np.count_nonzero(vecs)}\")\n",
    "    print(f\"Max absolute value in result: {np.max(np.abs(vecs.data))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdm - numpy\n",
    "\n",
    "min_eigenvalues_np = []\n",
    "min_rdms_np =[]\n",
    "rdm_eigenvalues_np = []\n",
    "\n",
    "for i in tqdm(red_indices):\n",
    "    # Construct the density matrix for each eigenvector\n",
    "    min_eigenvector = eigenvectors[:, i]\n",
    "    density_matrix = np.outer(min_eigenvector, min_eigenvector.conj())\n",
    "\n",
    "    # Trace out qubits\n",
    "    traced_out_density_matrix = ptrace_numpy(density_matrix, keep_qubits, [[2]*N, [2]*N])\n",
    "\n",
    "    # Diagonalize the traced-out density matrix\n",
    "    eigenvalues_traced, eigenvectors_traced = np.linalg.eigh(traced_out_density_matrix)\n",
    "\n",
    "    # Find the minimum eigenvalue of the traced-out density matrix\n",
    "    min_eigenvalue = np.min(eigenvalues_traced) \n",
    "    min_eigenvalues_np.append(min_eigenvalue)\n",
    "\n",
    "    if min_eigenvalue < 1e-16:\n",
    "        min_rdms_np.append(traced_out_density_matrix) #store the scarred rdms - step needed for optimization\n",
    "        rdm_eigenvalues_np.append(eigenvalues_traced) #store the eigenvalues of the scarred rdms\n",
    "\n",
    "# numpy\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 1e-16\n",
    "min_eigenvalues_np = np.array(min_eigenvalues_np)\n",
    "\n",
    "# Count points with y-component less than the threshold\n",
    "count = sum(1 for y in min_eigenvalues_np if y < threshold)\n",
    "\n",
    "# Highlight points with y-component less than the threshold in red\n",
    "colors = ['red' if y < threshold else 'C0' for y in min_eigenvalues_np]\n",
    "\n",
    "# Plot the minimum eigenvalue of the traced-out density matrix as a function of the eigenvalue of the Hamiltonian H\n",
    "plt.scatter(eigenvalues, np.abs(min_eigenvalues_np), color=colors, s=1)\n",
    "\n",
    "plt.xlabel('Eigenvalue of H')\n",
    "plt.ylabel(r\"$\\lambda_{\\text{min}}$\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "# Find indices of red points - numpy\n",
    "red_indices_np = [i for i, y in enumerate(min_eigenvalues_np) if y < threshold]\n",
    "print(f'Indices of red points: {red_indices_np}')\n",
    "\n",
    "red_eigenvectors_np = []\n",
    "\n",
    "# Print eigenvalues, RDM minimum eigenvalues, and scalar products of eigenvectors for red points\n",
    "for i,ind in enumerate(red_indices_np):\n",
    "    print(i, ind)\n",
    "    print(f'Eigenvalue: {eigenvalues[ind]}, rdm Minimum Eigenvalue: {min_eigenvalues_np[ind]}')\n",
    "    print(f'Eigenvector {ind}: rdm rank: {np.linalg.matrix_rank(min_rdms_np[i])}')\n",
    "    red_eigenvectors_np.append(eigenvectors[:, ind])\n",
    "\n",
    "#for i in range(len(red_indices_np)):\n",
    "#    for j in range(len(red_indices_np)):\n",
    "#        idx1, idx2 = red_indices_np[i], red_indices_np[j]\n",
    "#        dot_product = np.dot(eigenvectors[:, idx1], np.conj(eigenvectors[:, idx2]))\n",
    "#        print(f\"Dot product between eigenvectors {idx1} and {idx2}: {dot_product}\")\n",
    "\n",
    "# Count entries of rdm_eigenvalues_np[i] - eigenvalues of the scarred rdms - that are non-zero\n",
    "counts_np = [np.sum(eigenvalues > 1e-16) for eigenvalues in rdm_eigenvalues_np]\n",
    "print(f'Counts of non-zero eigenvalues of the scarred rdms: {counts_np}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_v = [0,2,4,5,8,9]\n",
    "half_h = [0,2,5,7,9,11]\n",
    "\n",
    "half = half_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bipartite entanglement entropy for all eigenstates in the full Hilbert space\n",
    "numpy_entropies = [entanglement_entropy(eigenvectors[:, i], half, N) for i in tqdm(range(eigenvectors.shape[1]))]\n",
    "#qutip_entropies = [entanglement_entropy_qutip(eigenvectors[:, i], half, N) for i in tqdm(range(eigenvectors.shape[1]))]\n",
    "#torch_entropies = [entanglement_entropy_torch(eigenvectors[:, i], half, N) for i in tqdm(range(eigenvectors.shape[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entropies of red points only\n",
    "print(\"Entropies of red points:\")\n",
    "for i in red_indices:\n",
    "    print(f'Index: {i}, Entropy: {numpy_entropies[i]}')\n",
    "\n",
    "# Plot entanglement entropy as a function of energy eigenvalues\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight points with y-component less than the threshold in red\n",
    "colors = ['red' if y < threshold else 'C0' for y in min_eigenvalues_np]\n",
    "\n",
    "# Plot entanglement entropy with highlighted points\n",
    "for i, color in enumerate(colors):\n",
    "    plt.plot(eigenvalues[i], numpy_entropies[i], 'o', color=color, markersize=2)\n",
    "\n",
    "# Add legend for red points only with text \"scars\"\n",
    "red_points = [i for i, color in enumerate(colors) if color == 'red']\n",
    "if red_points:\n",
    "    plt.plot([], [], 'o', color='red', label='Scars')\n",
    "\n",
    "plt.xlabel('Eigenvalue of H')\n",
    "plt.ylabel('Entanglement entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entropies of red points only\n",
    "print(\"Entropies of red points:\")\n",
    "for i in red_indices:\n",
    "    print(f'Index: {i}, Entropy: {numpy_entropies[i]}')\n",
    "\n",
    "# Find the red index with the lowest entanglement entropy\n",
    "min_entropy_idx = min(red_indices_np, key=lambda i: numpy_entropies[i])\n",
    "min_entropy_val = numpy_entropies[min_entropy_idx]\n",
    "\n",
    "# Plot entanglement entropy as a function of energy eigenvalues\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Highlight points with y-component less than the threshold in red\n",
    "colors = ['red' if y < threshold else 'C0' for y in min_eigenvalues_np]\n",
    "\n",
    "# Plot blue points first\n",
    "for i, color in enumerate(colors):\n",
    "    if color != 'red':\n",
    "        plt.plot(eigenvalues[i], numpy_entropies[i], 'o', color=color, markersize=2)\n",
    "# Then plot red points on top\n",
    "for i, color in enumerate(colors):\n",
    "    if color == 'red':\n",
    "        plt.plot(eigenvalues[i], numpy_entropies[i], 'o', color=color, markersize=4)\n",
    "\n",
    "# Plot entanglement entropy with highlighted points\n",
    "#for i, color in enumerate(colors):\n",
    "#    size = 3 if color == 'red' else 2\n",
    "#    plt.plot(eigenvalues[i], numpy_entropies[i], 'o', color=color, markersize=size)\n",
    "\n",
    "# Mark the scar with the lowest entanglement entropy with a yellow cross\n",
    "plt.plot(eigenvalues[min_entropy_idx], min_entropy_val, 'x', color='red', markersize=8, label=f'Min EE (idx={min_entropy_idx + 1})')\n",
    "\n",
    "# Add legend for red points only with text \"scars\"\n",
    "red_points = [i for i, color in enumerate(colors) if color == 'red']\n",
    "if red_points:\n",
    "    plt.plot([], [], 'o', color='red', markersize=4, label='Scars')\n",
    "\n",
    "plt.xlabel('Eigenvalue of H')\n",
    "plt.ylabel('Entanglement entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## PROPERTIES OF EACH SCARRED STATE AND THEIR RDM ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the number of dependent columns and their indices for each matrix\n",
    "dependent_columns_info = []\n",
    "\n",
    "for idx, rdm in enumerate(min_rdms_np):\n",
    "    print(f\"RDM Index: {idx}\")\n",
    "    \n",
    "    # Perform QR decomposition with column pivoting\n",
    "    Q, R, pivot_indices = qr(rdm, pivoting=True)\n",
    "    \n",
    "    # Determine rank using a threshold on the diagonal of R\n",
    "    tol = 1e-12\n",
    "    rank = np.sum(np.abs(np.diag(R)) > tol)\n",
    "    \n",
    "    # Calculate the number of dependent columns\n",
    "    num_dependent_columns = 16 - rank\n",
    "    \n",
    "    # Identify dependent columns\n",
    "    dependent_columns = sorted(set(range(16)) - set(pivot_indices[:rank]))\n",
    "    dependent_columns_info.append((num_dependent_columns, dependent_columns))\n",
    "    \n",
    "    print(f\"Number of dependent columns: {num_dependent_columns}\")\n",
    "    print(f\"Dependent columns indices: {dependent_columns}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print the results\n",
    "print(\"Dependent columns info for each matrix:\", dependent_columns_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the null space information for each matrix\n",
    "null_spaces_info = []\n",
    "\n",
    "for idx, rdm in enumerate(min_rdms_np):\n",
    "    print(f\"RDM Index: {idx}\")\n",
    "    \n",
    "    # Compute the null space of the RDM\n",
    "    null_space_rdm = null_space(rdm)\n",
    "    \n",
    "    # Store the null space information\n",
    "    null_spaces_info.append(null_space_rdm)\n",
    "    \n",
    "    # Print the null space dimensions and basis vectors\n",
    "    print(f\"Null space dimension: {null_space_rdm.shape[1]}\")\n",
    "    print(f\"Null space basis vectors:\\n{null_space_rdm}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Apply the RDM to each null basis vector\n",
    "    for i, basis_vector in enumerate(null_space_rdm.T):  # Transpose to iterate over columns\n",
    "        result = np.dot(rdm, basis_vector)\n",
    "        print(f\"RDM {idx}, Null Basis Vector {i}:\")\n",
    "        print(f\"Result: {result}\")\n",
    "        print(f\"Norm of Result: {np.linalg.norm(result)}\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Null space information for each matrix computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each RDM, check if any row in its null space basis matrix is (close to) zero\n",
    "for idx, rdm in enumerate(min_rdms_np):\n",
    "    null_space_rdm = null_space(rdm)\n",
    "    for i, row in enumerate(null_space_rdm):\n",
    "        if np.allclose(row, 0, atol=1e-12):\n",
    "            print(f\"RDM {idx}: Row {i} in null space is (close to) zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct rows in the null space basis of RDM 0\n",
    "rdm = min_rdms_np[4]\n",
    "null_space_rdm = null_space(rdm)\n",
    "\n",
    "# Use np.allclose to group rows that are numerically identical\n",
    "distinct_rows = []\n",
    "for i, row in enumerate(null_space_rdm):\n",
    "    is_new = True\n",
    "    for drow in distinct_rows:\n",
    "        if np.allclose(row, drow, atol=1e-12):\n",
    "            is_new = False\n",
    "            break\n",
    "    if is_new:\n",
    "        distinct_rows.append(row)\n",
    "\n",
    "print(f\"Number of distinct rows in null space basis of RDM 0: {len(distinct_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the binary basis states for each group of identical rows in the null space basis of RDM 0 (5 spins)\n",
    "# Also print the smallest row index for each group\n",
    "\n",
    "def index_to_binary(index, num_qubits):\n",
    "    return format(index, f'0{num_qubits}b')\n",
    "\n",
    "rdm = min_rdms_np[0]\n",
    "null_space_rdm = null_space(rdm)\n",
    "\n",
    "# Group row indices by their unique row (up to numerical tolerance)\n",
    "groups = []\n",
    "group_indices = []\n",
    "\n",
    "for i, row in enumerate(null_space_rdm):\n",
    "    found = False\n",
    "    for g, grow in enumerate(groups):\n",
    "        if np.allclose(row, grow, atol=1e-12):\n",
    "            group_indices[g].append(i)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        groups.append(row)\n",
    "        group_indices.append([i])\n",
    "\n",
    "print(f\"Number of distinct rows: {len(groups)}\")\n",
    "for indices in group_indices:\n",
    "    basis_states = [index_to_binary(idx, 5) for idx in indices]\n",
    "    min_row = min(indices)\n",
    "    print(f\"Row {min_row}: Basis states: {', '.join(basis_states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the null space information for each matrix\n",
    "null_spaces_info = []\n",
    "\n",
    "for idx, rdm in enumerate(min_rdms_np):\n",
    "    print(f\"RDM Index: {idx}\")\n",
    "    \n",
    "    # Compute the null space of the RDM\n",
    "    null_space_rdm = null_space(rdm)\n",
    "    \n",
    "    # Store the null space information\n",
    "    null_spaces_info.append(null_space_rdm)\n",
    "    \n",
    "    # Print the null space dimensions and basis vectors\n",
    "    print(f\"Null space dimension: {null_space_rdm.shape[1]}\")\n",
    "    #print(f\"Null space basis vectors:\\n{null_space_rdm}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Apply the RDM to each null basis vector\n",
    "    #for i, basis_vector in enumerate(null_space_rdm.T):  # Transpose to iterate over columns\n",
    "        #result = np.dot(rdm, basis_vector)\n",
    "        #print(f\"RDM {idx}, Null Basis Vector {i}:\")\n",
    "        #print(f\"Result: {result}\")\n",
    "        #print(f\"Norm of Result: {np.linalg.norm(result)}\")\n",
    "    \n",
    "    # Check for repeating rows in the null space basis vectors\n",
    "    print(f\"Repeating rows in the null space basis vectors for RDM {idx}:\")\n",
    "    for i in range(null_space_rdm.shape[0]):\n",
    "        for j in range(i + 1, null_space_rdm.shape[0]):\n",
    "            if np.allclose(null_space_rdm[i, :], null_space_rdm[j, :], atol=1e-12):  # Compare rows with a tolerance\n",
    "                print(f\"Row {i} is identical to Row {j}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over scarred eigenvectors\n",
    "for i, eigenvector in enumerate(red_eigenvectors_np):\n",
    "    # Print the entire eigenvector\n",
    "    non_zero_count = 0  # Counter for non-zero components\n",
    "    print(f\"Full Eigenvector {i}:\")\n",
    "    for index, component in enumerate(eigenvector):\n",
    "        # Set components of the order 10^-12 or smaller to zero\n",
    "        if np.abs(component) < 1e-12: # I noticed that  most components are of the order 10^-13 or smaller - only a handful are mucgh larger\n",
    "            eigenvector[index] = 0.0\n",
    "        else:\n",
    "            non_zero_count += 1\n",
    "        binary_basis = index_to_binary(index, N)\n",
    "        print(f\"{binary_basis}: {eigenvector[index]}\")\n",
    "\n",
    "    print(f\"Total Non-Zero Components in Scarred Eigenvector {i}: {non_zero_count}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over scarred eigenvectors\n",
    "for i, eigenvector in enumerate(red_eigenvectors_np):\n",
    "    print(f\"Scarred Eigenvector {i}:\")\n",
    "    \n",
    "    # Dictionary to track processed coefficients\n",
    "    processed_coeffs = set()\n",
    "    \n",
    "    # Iterate over components of the eigenvector\n",
    "    for index, component in enumerate(eigenvector):\n",
    "        if np.abs(component) > 1e-12:  # Check if the component is non-zero\n",
    "            binary_basis = index_to_binary(index, N)\n",
    "            coeff = component\n",
    "            \n",
    "            # Skip if this coefficient (or its negative) has already been processed\n",
    "            if coeff in processed_coeffs or -coeff in processed_coeffs:\n",
    "                continue\n",
    "            \n",
    "            # Count the number of 1's in the binary representation\n",
    "            num_ones = binary_basis.count('1')\n",
    "            \n",
    "            # Print the coefficient, binary representation, and number of 1's\n",
    "            print(f\"  Coefficient: {coeff}\")\n",
    "            print(f\"    Binary Representation: {binary_basis}\")\n",
    "            print(f\"    Elements [0, 4, 5, 9]: {binary_basis[0]}, {binary_basis[4]}, {binary_basis[5]}, {binary_basis[9]}\")\n",
    "            print(f\"    Number of 1's: {num_ones}\")\n",
    "            \n",
    "            # Check for the negative of the coefficient\n",
    "            for j, other_component in enumerate(eigenvector):\n",
    "                if np.abs(other_component + coeff) < 1e-10:  # Compare with tolerance\n",
    "                    binary_basis_neg = index_to_binary(j, N)\n",
    "                    num_ones_neg = binary_basis_neg.count('1')\n",
    "                    print(f\"  Negative Coefficient: {-coeff}\")\n",
    "                    print(f\"    Binary Representation: {binary_basis_neg}\")\n",
    "                    print(f\"    Elements [0, 4, 5, 9]: {binary_basis_neg[0]}, {binary_basis_neg[4]}, {binary_basis_neg[5]}, {binary_basis_neg[9]}\")\n",
    "                    print(f\"    Number of 1's: {num_ones_neg}\")\n",
    "                    break\n",
    "            \n",
    "            # Mark this coefficient and its negative as processed\n",
    "            processed_coeffs.add(coeff)\n",
    "            processed_coeffs.add(-coeff)\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store even swaps numbers\n",
    "even_swaps_numbers = []\n",
    "\n",
    "# Dictionary to track spin exchange arrays and their counts\n",
    "spin_exchange_counts = {}\n",
    "\n",
    "# Iterate over scarred eigenvectors\n",
    "for i, eigenvector in enumerate(red_eigenvectors_np):\n",
    "    print(f\"Scarred Eigenvector {i}:\")\n",
    "\n",
    "    # Dictionary to track processed coefficients\n",
    "    processed_coeffs = set()\n",
    "\n",
    "    # Iterate over components of the eigenvector\n",
    "    for index, component in enumerate(eigenvector):\n",
    "        if np.abs(component) > 1e-12:  # Check if the component is non-zero\n",
    "            binary_basis = index_to_binary(index, N)\n",
    "            coeff = component\n",
    "\n",
    "            # Skip if this coefficient (or its negative) has already been processed\n",
    "            if coeff in processed_coeffs or -coeff in processed_coeffs:\n",
    "                continue\n",
    "\n",
    "            # Check for the negative of the coefficient\n",
    "            for j, other_component in enumerate(eigenvector):\n",
    "                if np.abs(other_component + coeff) < 1e-10:  # Compare with tolerance\n",
    "                    binary_basis_neg = index_to_binary(j, N)\n",
    "\n",
    "                    # Identify the spins that are exchanged\n",
    "                    spin_exchange = [\n",
    "                        k for k in range(N) if binary_basis[k] != binary_basis_neg[k]\n",
    "                    ]\n",
    "\n",
    "                    # Compute swaps number\n",
    "                    swaps_number = len(spin_exchange) // 2\n",
    "\n",
    "                    # Check if swaps number is odd\n",
    "                    is_odd = swaps_number % 2 == 1\n",
    "\n",
    "                    print(f\"  Coefficient: {coeff}\")\n",
    "                    print(f\"    Binary Representation: {binary_basis}\")\n",
    "                    print(f\"  Negative Coefficient: {-coeff}\")\n",
    "                    print(f\"    Binary Representation: {binary_basis_neg}\")\n",
    "                    print(f\"    Spin Exchange: {spin_exchange}\")\n",
    "                    print(f\"    Swaps Number: {swaps_number} (Odd: {is_odd})\")\n",
    "\n",
    "                    # Add to even swaps numbers if swaps number is even\n",
    "                    if not is_odd:\n",
    "                        even_swaps_numbers.append(swaps_number)\n",
    "\n",
    "                    # Track spin exchange arrays\n",
    "                    spin_exchange_tuple = tuple(spin_exchange)\n",
    "                    if spin_exchange_tuple in spin_exchange_counts:\n",
    "                        spin_exchange_counts[spin_exchange_tuple] += 1\n",
    "                    else:\n",
    "                        spin_exchange_counts[spin_exchange_tuple] = 1\n",
    "\n",
    "                    break\n",
    "\n",
    "            # Mark this coefficient and its negative as processed\n",
    "            processed_coeffs.add(coeff)\n",
    "            processed_coeffs.add(-coeff)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Print the list of even swaps numbers\n",
    "print(\"Even swaps numbers:\", even_swaps_numbers)\n",
    "\n",
    "# Check for repeated spin exchange arrays\n",
    "repeated_spin_exchanges = [\n",
    "    spin_exchange for spin_exchange, count in spin_exchange_counts.items() if count > 1\n",
    "]\n",
    "if repeated_spin_exchanges:\n",
    "    print(\"Repeated spin exchange arrays:\", repeated_spin_exchanges)\n",
    "else:\n",
    "    print(\"No repeated spin exchange arrays found.\")\n",
    "\n",
    "print(len(spin_exchange_counts), \"spin exchange arrays found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_scar\n",
    "\n",
    "non_zero_count = 0  # Counter for non-zero components\n",
    "\n",
    "for index, component in enumerate(min_scar):\n",
    "    # Set components of the order 10^-12 or smaller to zero\n",
    "    if np.abs(component) < 1e-12: # I noticed that  most components are of the order 10^-13 or smaller - only a handful are mucgh larger\n",
    "        min_scar[index] = 0.0\n",
    "    else:\n",
    "        non_zero_count += 1\n",
    "    binary_basis = index_to_binary(index, N)\n",
    "    print(f\"{binary_basis}: {np.real(min_scar[index])}\")\n",
    "\n",
    "print(f\"Total Non-Zero Components in min_scar: {non_zero_count}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if other states have 280 non-zero components - it's only the 5 scars!\n",
    "\n",
    "# Count distinct non_zero_count values\n",
    "distinct_counts = set()\n",
    "\n",
    "# Counter to track occurrences of each non_zero_count\n",
    "non_zero_count_occurrences = Counter()\n",
    "\n",
    "# Iterate over all eigenvectors\n",
    "for i in range(eigenvectors.shape[1]):\n",
    "    eigenvector = eigenvectors[:, i]\n",
    "    non_zero_count = 0  # Counter for non-zero components\n",
    "    \n",
    "    # Count non-zero components\n",
    "    for component in eigenvector:\n",
    "        if np.abs(component) >= 1e-12:\n",
    "            non_zero_count += 1\n",
    "    \n",
    "    # Update the counter\n",
    "    non_zero_count_occurrences[non_zero_count] += 1\n",
    "    print(f\"Total Non-Zero Components in Eigenvector {i}: {non_zero_count}\")\n",
    "\n",
    "# Print the number of distinct non_zero_count values\n",
    "print(f\"Number of distinct non_zero_count values: {len(non_zero_count_occurrences)}\")\n",
    "print(f\"Distinct non_zero_count values: {sorted(non_zero_count_occurrences.keys())}\")\n",
    "\n",
    "# Print how many times each distinct non_zero_count appears\n",
    "print(\"Occurrences of each non_zero_count:\")\n",
    "for count, occurrences in sorted(non_zero_count_occurrences.items()):\n",
    "    print(f\"Non-Zero Count: {count}, Occurrences: {occurrences}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
